{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/CcP3jQaIPMltFmZDQXLH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatheusMRech/ETL-Dataiku-DSS/blob/master/MATLAB_Major_Adverse_Cardiovascular_Events_%2B_Machine_Learning_Algorithm_PARTE_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Predicting  Major Adverse Cardiovascular Events after Liver Transplantation using a Machine Learning *"
      ],
      "metadata": {
        "id": "PeriX0NSZC8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sessão de métodos descrita abaixo do código**"
      ],
      "metadata": {
        "id": "_8_8hpQOYWHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DADOS UTILIZADOS: https://www.dropbox.com/s/n1g65lfzvz6d48a/mace_new.csv?dl=0\n",
        "\n",
        "\n",
        "% Load required packages\n",
        "pkg load dataframe;\n",
        "pkg load io;\n",
        "pkg load statistics;\n",
        "pkg load optim;\n",
        "pkg load nan;\n",
        "pkg load imbalanced-learn;\n",
        "\n",
        "% Load your patient data as a structure array\n",
        "response = jsondecode(fileread('path_to_your_json_file'));\n",
        "df = struct2table(response.mace);\n",
        "\n",
        "% Select the desired columns\n",
        "columns = {'age', 'race', 'sex', 'weight', 'height', 'leve', 'ascitis', 'pbe', 'shp', 'ppl', 'portalveintromb', 'eps', 'hda_ve', ...\n",
        "    'shr', 'atb_24h', 'inpatient_48h', 'hd_pre', 'chc', 'hb', 'ht', 'leuc', 'plaq', 'bt', 'bd', 'cr', 'ur', 'tp', 'rni', ...\n",
        "    'na', 'k', 'alb', 'tgo', 'tgp', 'ggt', 'fa', 'afp', 'group', 'rh', 'glic', 'smoker', 'icc', 'angioplast', 'dlp', 'hf', ...\n",
        "    'hypertension', 'iam', 'stroke', 'dm', 'fe', 'ae', 'ved', 'ves', 'trocavalvar', 'no_invasive_method', 'dinamic_alteration'};\n",
        "\n",
        "df = df(:, columns);\n",
        "\n",
        "% Convert the categorical variables to numerical if necessary\n",
        "% df = dummyvar(df, 'sex', 'race', ...);\n",
        "\n",
        "% Split the data into features and target\n",
        "X = df(:, 1:end-1);\n",
        "y = df.mace;\n",
        "\n",
        "% Impute missing values using kNN\n",
        "X = knnimpute(X, 10);\n",
        "\n",
        "% Split the data into training (60%), validation (20%), and testing (20%) sets\n",
        "[trainInd, valInd, testInd] = dividerand(height(df), 0.6, 0.2, 0.2);\n",
        "X_train = X(trainInd, :);\n",
        "y_train = y(trainInd);\n",
        "X_val = X(valInd, :);\n",
        "y_val = y(valInd);\n",
        "X_test = X(testInd, :);\n",
        "y_test = y(testInd);\n",
        "\n",
        "% Handle class imbalance using SMOTE\n",
        "[X_train_resampled, y_train_resampled] = smote(X_train, y_train, 1);\n",
        "\n",
        "% Build and train the XGBoost model with tuned parameters\n",
        "xgb = fitcensemble(X_train_resampled, y_train_resampled, 'Method', 'AdaBoostM2', ...\n",
        "    'Learner', templateTree('MaxNumSplits', 5), 'NumLearningCycles', 100);\n",
        "\n",
        "% Build and train the logistic regression model\n",
        "glm = fitglm(X_train_resampled, y_train_resampled, 'Distribution', 'binomial', 'link', 'logit', 'MaxIter', 1000);\n",
        "\n",
        "% Perform 5-fold cross-validation to find the best ensemble weights\n",
        "n_splits = 5;\n",
        "cv = cvpartition(y_train_resampled, 'KFold', n_splits);\n",
        "best_weights = [0, 0];\n",
        "best_auc = 0;\n",
        "\n",
        "for weight_xgb = linspace(0, 1, 11)\n",
        "    weight_glm = 1 - weight_xgb;\n",
        "    auc_sum = 0;\n",
        "    \n",
        "    for k = 1:n_splits\n",
        "        train_idx = training(cv, k);\n",
        "        val_idx = test(cv, k);\n",
        "        \n",
        "        X_train_k = X_train_resampled(train_idx, :);\n",
        "        y_train_k = y_train_resampled(train_idx);\n",
        "        X\n",
        "\n",
        "\n",
        "\n",
        "    for train_index, val_index in skf.split(X_train_resampled, y_train_resampled):\n",
        "        X_train_cv, X_val_cv = X_train_resampled[train_index], X_train_resampled[val_index]\n",
        "        y_train_cv, y_val_cv = y_train_resampled[train_index], y_train_resampled[val_index]\n",
        "\n",
        "        xgb.fit(X_train_cv, y_train_cv)\n",
        "        glm.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "        y_val_pred_xgb = xgb.predict_proba(X_val_cv)[:, 1]\n",
        "        y_val_pred_glm = glm.predict_proba(X_val_cv)[:, 1]\n",
        "\n",
        "        y_val_pred_ensemble = weight_xgb * y_val_pred_xgb + weight_glm * y_val_pred_glm\n",
        "        auc_sum += roc_auc_score(y_val_cv, y_val_pred_ensemble)\n",
        "\n",
        "    auc_avg = auc_sum / n_splits\n",
        "    if auc_avg > best_auc:\n",
        "        best_auc = auc_avg\n",
        "        best_weights = (weight_xgb, weight_glm)\n",
        "\n",
        "# Apply the best ensemble weights\n",
        "weight_xgb, weight_glm = best_weights\n",
        "y_val_pred_xgb = xgb.predict_proba(X_val)[:, 1]\n",
        "y_val_pred_glm = glm.predict_proba(X_val)[:, 1]\n",
        "y_val_pred_ensemble = weight_xgb * y_val_pred_xgb + weight_glm * y_val_pred_glm\n",
        "\n",
        "# Evaluate the performance of the ensemble model\n",
        "auc_val = roc_auc_score(y_val, y_val_pred_ensemble)\n",
        "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_pred_ensemble)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr_val, tpr_val, label='ROC curve (area = %0.2f)' % auc_val)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic (ROC) Curve - Validation Set')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Apply the ensemble model to the test set\n",
        "y_test_pred_xgb = xgb.predict_proba(X_test)[:, 1]\n",
        "y_test_pred_glm = glm.predict_proba(X_test)[:, 1]\n",
        "y_test_pred_ensemble = weight_xgb * y_test_pred_xgb + weight_glm * y_test_pred_glm\n",
        "\n",
        "# Calculate performance metrics on the test set\n",
        "auc_test = roc_auc_score(y_test, y_test_pred_ensemble)\n",
        "accuracy = accuracy_score(y_test, y_test_pred_ensemble.round())\n",
        "precision = precision_score(y_test, y_test_pred_ensemble.round())\n",
        "recall = recall_score(y_test, y_test_pred_ensemble.round())\n",
        "f1 = f1_score(y_test, y_test_pred_ensemble.round())\n",
        "\n",
        "print(\"Test set metrics:\")\n",
        "print(\"AUC: \", auc_test)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)\n",
        "```\n",
        "\n",
        "#MODEL INTERPRETABILITY - MUITO IMPORTANTE\n",
        "\n",
        "#There is a package in MATLAB called LIME (Local Interpretable Model-agnostic Explanations) that can provide model interpretability for an XGBoost model. LIME is a popular method that can be used to explain the predictions of any classifier or regression model by approximating it locally with an interpretable model.\n",
        "\n",
        "To use LIME in MATLAB, you can download the MATLAB implementation provided by the authors of the LIME paper. It is available on GitHub:\n",
        "\n",
        "https://github.com/marcotcr/lime/tree/master/lime\n",
        "\n",
        "Clone the repository or download the 'lime' folder from the link above and add it to your MATLAB path. Once you have done that, you can use LIME to interpret the predictions of the XGBoost model.\n",
        "\n",
        "Here's an example of how to use LIME with your XGBoost model:\n",
        "\n",
        "First, you need to define a prediction function that takes in a data matrix and outputs the predicted probabilities for each instance. For your XGBoost model, the function should look like t\n",
        "\n",
        "\n",
        "\n",
        "function pred_probs = xgb_predict_fn(X)\n",
        "    % Load the saved XGBoost model\n",
        "    load('xgb_model.mat', 'xgb');\n",
        "\n",
        "    % Make predictions\n",
        "    pred_probs = predict(xgb, X, 'PredictMethod', 'ensemble');\n",
        "end\n",
        "\n",
        "#Next, you need to instantiate a LimeTabularExplainer object and use it to explain individual predictions. Here's an example:\n",
        "\n",
        "% Import LIME\n",
        "import lime.*;\n",
        "\n",
        "% Choose an instance for which you want to explain the prediction\n",
        "instance_idx = 1;\n",
        "instance = X_test(instance_idx, :);\n",
        "\n",
        "% Create a LimeTabularExplainer object\n",
        "explainer = LimeTabularExplainer(X_train, 'PredictFcn', @xgb_predict_fn);\n",
        "\n",
        "% Explain the prediction for the chosen instance\n",
        "num_features = 5; % Number of features to display in the explanation\n",
        "explanation = explainer.explain_instance(instance, 'NumFeatures', num_features);\n",
        "\n",
        "% Display the explanation\n",
        "disp(explanation);\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-RcrdmngVLzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**METODOS E MATERIAIS**"
      ],
      "metadata": {
        "id": "67eCTXUOZnh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Methodology\n",
        "In this study, we aim to predict the occurrence of major adverse cardiac events (MACE) in patients using clinical and demographic features. We developed a machine learning pipeline using supervised learning techniques, including logistic regression and gradient-boosted decision trees (XGBoost) in Python programming language.\n",
        "\n",
        "3.1. Data preprocessing\n",
        "We first loaded the dataset and selected relevant features. The dataset was preprocessed by converting categorical variables to numerical values and imputing missing values using k-nearest neighbors (kNN) imputation. The dataset was then divided into three subsets: 60% for training, 20% for validation, and 20% for testing.\n",
        "\n",
        "3.2. Handling class imbalance\n",
        "Due to the imbalance in the MACE distribution, we utilized the Synthetic Minority Over-sampling Technique (SMOTE) to balance the classes in the training data.\n",
        "\n",
        "3.3. Model training\n",
        "Two models, logistic regression and XGBoost, were trained using the preprocessed and resampled training data. The XGBoost hyperparameters were tuned to optimize the model's performance.\n",
        "\n",
        "3.4. Ensemble model\n",
        "A 5-fold cross-validation was performed to find the best combination of ensemble weights for logistic regression and XGBoost models, aiming to improve the overall performance. The ensemble model was created by combining the predictions of the two models with the determined weights.\n",
        "\n",
        "3.5. Model evaluation\n",
        "The performance of the ensemble model was evaluated using various metrics such as ROC-AUC, accuracy, precision, recall, and F1-score. The model's performance was assessed on both the validation and test datasets to estimate its generalization to unseen data.\n",
        "\n",
        "3.6. Feature importance and model interpretability\n",
        "To gain insights into the most important features contributing to the predictions, we analyzed feature importance in the XGBoost model. Additionally, we investigated the relationship between the features and the predicted outcomes using partial dependence plots.\n",
        "\n",
        "In summary, we developed a machine learning pipeline to predict MACE in patients using logistic regression and XGBoost models. The models were trained using a balanced training dataset and combined through an ensemble approach. The performance of the ensemble model was evaluated using various metrics, and feature importance analysis was conducted to understand the main drivers of the predictions."
      ],
      "metadata": {
        "id": "3JoX8PqXVivx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation and evaluation of the XGBoost and Logistic Regression models FOR OUT-OF-SAMPLE ESTIMATION\n",
        "\n",
        "1. `StratifiedKFold`: The `StratifiedKFold` object is used to create a stratified cross-validation. Stratified cross-validation ensures that the class proportions are maintained in each train and validation set.\n",
        "\n",
        "2. Validating the models and finding the best weight combination: The code uses a loop to test different weight combinations to create an ensemble model from the XGBoost and Logistic Regression models. The AUC (Area under the ROC curve) metric is calculated for each weight combination, and then the combination with the highest average AUC across the cross-validation splits is selected as the best.\n",
        "\n",
        "3. Applying the best weights: After finding the best weight combination, it is applied to create an ensemble model of the XGBoost and Logistic Regression models. The predicted probabilities from both models are combined using the best weights found.\n",
        "\n",
        "4. Evaluation of the ensemble model on the validation set: The AUC, ROC curve, and plot are calculated and displayed for the validation set.\n",
        "\n",
        "5. Applying the ensemble model on the test set: The final ensemble model is applied to the test set, and the predicted probabilities are calculated.\n",
        "\n",
        "6. Performance metrics on the test set: The AUC, accuracy, precision, recall, and F1 score are calculated for the test set, and the results are displayed.\n",
        "\n",
        "After adding this part of the code, you will have a complete pipeline that covers data cleaning and preparation, modeling, cross-validation, and model evaluation."
      ],
      "metadata": {
        "id": "iY5fGKwuVlh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________________________________**OUTRA VERSÃO**_____________________________"
      ],
      "metadata": {
        "id": "fkjIGZSmc2ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here is a complete MATLAB code that combines all of the previous snippets and performs the necessary steps for your project. The code first preprocesses the data, handles class imbalance, trains an XGBoost model, and then uses LIME for model interpretability. The description for incorporating this in your methods section is provided below the code.\n",
        "\n",
        "MATLAB\n",
        "**"
      ],
      "metadata": {
        "id": "-E7wq0u5dgVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "% Load the data\n",
        "% Assuming that the data is in a CSV file called 'patient_data.csv'\n",
        "data = readtable('patient_data.csv');\n",
        "\n",
        "% Preprocess the data\n",
        "% Perform necessary preprocessing steps, such as converting categorical variables to numerical, scaling, and imputing missing values\n",
        "\n",
        "% Split the data into features (X) and target (y)\n",
        "X = data(:, 1:end-1);\n",
        "y = data.mace;\n",
        "\n",
        "% Split the data into training (60%), validation (20%), and testing (20%) sets\n",
        "rng(42); % Set random seed for reproducibility\n",
        "cv = cvpartition(y, 'Holdout', 0.4);\n",
        "X_train = X(training(cv), :);\n",
        "y_train = y(training(cv), :);\n",
        "X_temp = X(test(cv), :);\n",
        "y_temp = y(test(cv), :);\n",
        "\n",
        "cv_temp = cvpartition(y_temp, 'Holdout', 0.5);\n",
        "X_val = X_temp(training(cv_temp), :);\n",
        "y_val = y_temp(training(cv_temp), :);\n",
        "X_test = X_temp(test(cv_temp), :);\n",
        "y_test = y_temp(test(cv_temp), :);\n",
        "\n",
        "% Handle class imbalance using SMOTE\n",
        "smote = smote(X_train, y_train, 'Verbose', true);\n",
        "X_train_resampled = smote.X_resampled;\n",
        "y_train_resampled = smote.Y_resampled;\n",
        "\n",
        "% Train the XGBoost model\n",
        "xgb = fitcensemble(X_train_resampled, y_train_resampled, 'Method', 'AdaBoostM1', 'Learner', templateTree('MaxNumSplits', 5));\n",
        "\n",
        "% Save the XGBoost model\n",
        "save('xgb_model.mat', 'xgb');\n",
        "\n",
        "% Model interpretability with LIME\n",
        "import lime.*;\n",
        "\n",
        "% Define a prediction function for the XGBoost model\n",
        "function pred_probs = xgb_predict_fn(X)\n",
        "    load('xgb_model.mat', 'xgb');\n",
        "    pred_probs = predict(xgb, X, 'PredictMethod', 'ensemble');\n",
        "end\n",
        "\n",
        "% Choose an instance for which you want to explain the prediction\n",
        "instance_idx = 1;\n",
        "instance = X_test(instance_idx, :);\n",
        "\n",
        "% Create a LimeTabularExplainer object\n",
        "explainer = LimeTabularExplainer(X_train, 'PredictFcn', @xgb_predict_fn);\n",
        "\n",
        "% Explain the prediction for the chosen instance\n",
        "num_features = 5;\n",
        "explanation = explainer.explain_instance(instance, 'NumFeatures', num_features);\n",
        "\n",
        "% Display the explanation\n",
        "disp(explanation);\n",
        "\n",
        "\n",
        "\n",
        "#For your methods section, you can include the following description:\n",
        "\n",
        "#In our study, we used MATLAB to preprocess the data, handle class imbalance using SMOTE, train an XGBoost model, and perform model interpretability with LIME. We first loaded the data and preprocessed it by converting categorical variables to numerical, scaling, and imputing missing values. The data was then split into training (60%), validation (20%), and testing (20%) sets.\n",
        "\n",
        "#o handle class imbalance, we employed the Synthetic Minority Over-sampling Technique (SMOTE) on the training set. We trained an XGBoost model using the AdaBoostM1 method with decision trees as weak learners. The tree depth was limited to five splits.\n",
        "\n",
        "#For model interpretability, we used the LIME (Local Interpretable Model-agnostic Explanations) package in MATLAB. LIME is a method that explains the predictions of any classifier by approximating it locally with an interpretable model. We defined a prediction function for our trained XGBoost model and created a LimeTabularExplainer object using the training data and our prediction function. We then selected an instance from the test set and used the `explain_instance` method to generate an explanation for the model's prediction on that instance. The explanation highlights the top `num_features` most important features contributing to the prediction, providing insight into the local behavior of the model. This approach enables us to better understand the model's decision-making process and increase its interpretability for individual predictions. In addition to LIME, other model interpretability methods such as SHAP (SHapley Additive exPlanations) and partial dependence plots (PDP) can also be used to enhance the understanding of the model's predictions. These methods can provide valuable insights into the global behavior of the model, feature importances, and the marginal effect of features on the predicted outcomes. By combining these interpretability techniques, we can gain a more comprehensive understanding of our XGBoost model and its predictions, ultimately enabling better decision-making and increased trust in the model's outputs."
      ],
      "metadata": {
        "id": "3sW2aClbd4gA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}